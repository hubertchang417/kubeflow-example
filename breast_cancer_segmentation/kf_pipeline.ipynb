{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import kfp\n",
    "from kfp import dsl\n",
    "import kfp.components as components\n",
    "from typing import NamedTuple\n",
    "from datetime import datetime\n",
    "\n",
    "def split_data():\n",
    "    import os\n",
    "    from glob import glob\n",
    "    from minio import Minio\n",
    "    import numpy as np\n",
    "    from PIL import Image\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    ## get data from minio\n",
    "    minio_client = Minio(\n",
    "       \"<minio_ep>\",\n",
    "        access_key=\"<minio_accK>\",\n",
    "        secret_key=\"<minio_secK>\",\n",
    "        secure=False\n",
    "    )\n",
    "    minio_bucket = \"kubeflow\"\n",
    "\n",
    "\n",
    "    ## get data from minio\n",
    "    print('Downloading data from minio...')\n",
    "    for f in minio_client.list_objects(minio_bucket, prefix=\"datasets/breast_cancer_segmentation/Dataset_BUSI_with_GT/\",recursive=True):\n",
    "        save_path = f.object_name.replace('datasets/breast_cancer_segmentation', '/tmp')\n",
    "        minio_client.fget_object(minio_bucket, f.object_name, save_path)\n",
    "\n",
    "    class BrestCancerData:\n",
    "        def __init__(self,size=256):\n",
    "            self._image_path = ''\n",
    "            self._mask_path = []\n",
    "            self._size = size\n",
    "\n",
    "        def set_img_path(self, image_path):\n",
    "            self._image_path = image_path\n",
    "        def set_mask_path(self, mask_path):\n",
    "            self._mask_path.append(mask_path)\n",
    "\n",
    "        def get_img_path(self):\n",
    "            return self._image_path\n",
    "        def get_mask_path(self):\n",
    "            return self._mask_path\n",
    "\n",
    "        def put_img(self):\n",
    "            if self._image_path:\n",
    "                img = Image.open(self._image_path).convert('RGB')\n",
    "                return img\n",
    "            else:\n",
    "                print('Please set the image path!')\n",
    "                return None\n",
    "        def put_mask(self):\n",
    "            if self._mask_path:\n",
    "                mask_lists = []\n",
    "                for mPath in self._mask_path:\n",
    "                    mask = Image.open(mPath).convert('L')\n",
    "                    mask = np.array(mask)\n",
    "                    mask_lists.append(mask)\n",
    "                sum_mask = np.clip(sum(mask_lists,np.zeros_like(mask_lists[0])), 0, 255)\n",
    "                return Image.fromarray(sum_mask.astype(np.uint8))\n",
    "            else:\n",
    "                print('Please set the mask path!')\n",
    "                return None\n",
    "\n",
    "    data_all = {}\n",
    "    unnormal_path = glob('/tmp/Dataset_BUSI_with_GT/benign/*.png') + glob('/tmp/Dataset_BUSI_with_GT/malignant/*.png')\n",
    "    for fPath in unnormal_path:\n",
    "        _, file_name = os.path.split(fPath)\n",
    "        file_name, _ = os.path.splitext(file_name)\n",
    "        stack = file_name.split('_')\n",
    "\n",
    "        if  data_all.get(stack[0], None) == None:\n",
    "            data_all[stack[0]] = BrestCancerData()\n",
    "\n",
    "        if len(stack) >= 2: # mask\n",
    "            data_all[stack[0]].set_mask_path(fPath)\n",
    "        else: # image\n",
    "            data_all[stack[0]].set_img_path(fPath)\n",
    "\n",
    "    name_lists = list(data_all.keys())\n",
    "    train_name, val_test_name = train_test_split(name_lists, test_size=0.2, random_state=42)\n",
    "    val_name, test_name = train_test_split(val_test_name, test_size=0.5, random_state=42)\n",
    "    ## save train_dataset\n",
    "    print('Saving train_dataset...')\n",
    "    for idx, name in enumerate(train_name):\n",
    "        img = data_all[name].put_img()\n",
    "        mask = data_all[name].put_mask()\n",
    "\n",
    "        sPath = f'/tmp/train_dataset/{idx+1}'\n",
    "        os.makedirs(sPath)\n",
    "        img.save(f'{sPath}/image.png')\n",
    "        mask.save(f'{sPath}/mask.png')\n",
    "        with open(f'{sPath}/info', 'w') as f:\n",
    "            f.write(f'Instance: {name}')\n",
    "\n",
    "    ## save val_dataset\n",
    "    print('Saving val_dataset...')\n",
    "    for idx, name in enumerate(val_name):\n",
    "        img = data_all[name].put_img()\n",
    "        mask = data_all[name].put_mask()\n",
    "\n",
    "        sPath = f'/tmp/val_dataset/{idx+1}'\n",
    "        os.makedirs(sPath)\n",
    "        img.save(f'{sPath}/image.png')\n",
    "        mask.save(f'{sPath}/mask.png')\n",
    "        with open(f'{sPath}/info', 'w') as f:\n",
    "            f.write(f'Instance: {name}')\n",
    "\n",
    "    ## save test_dataset\n",
    "    print('Saving test_dataset...')\n",
    "    for idx, name in enumerate(test_name):\n",
    "        img = data_all[name].put_img()\n",
    "        mask = data_all[name].put_mask()\n",
    "\n",
    "        sPath = f'/tmp/test_dataset/{idx+1}'\n",
    "        os.makedirs(sPath)\n",
    "        img.save(f'{sPath}/image.png')\n",
    "        mask.save(f'{sPath}/mask.png')\n",
    "        with open(f'{sPath}/info', 'w') as f:\n",
    "            f.write(f'Instance: {name}')\n",
    "\n",
    "    def upload_local_directory_to_minio(local_path, bucket_name, minio_path):\n",
    "        assert os.path.isdir(local_path)\n",
    "\n",
    "        for local_file in glob(local_path + '/**'):\n",
    "            local_file = local_file.replace(os.sep, \"/\") # Replace \\ with / on Windows\n",
    "            if not os.path.isfile(local_file):\n",
    "                upload_local_directory_to_minio(\n",
    "                    local_file, bucket_name, minio_path + \"/\" + os.path.basename(local_file))\n",
    "            else:\n",
    "                remote_path = os.path.join(\n",
    "                    minio_path, local_file[1 + len(local_path):])\n",
    "                remote_path = remote_path.replace(\n",
    "                    os.sep, \"/\")  # Replace \\ with / on Windows\n",
    "                minio_client.fput_object(bucket_name, remote_path, local_file)\n",
    "\n",
    "    ## upload train_dataset to minio\n",
    "    print('Uploading train_dataset to minio...')\n",
    "    upload_local_directory_to_minio('/tmp/train_dataset', minio_bucket, 'datasets/breast_cancer_segmentation/Dataset_BUSI_with_GT/train_dataset')\n",
    "    ## upload val_dataset to minio\n",
    "    print('Uploading val_dataset to minio...')\n",
    "    upload_local_directory_to_minio('/tmp/val_dataset', minio_bucket, 'datasets/breast_cancer_segmentation/Dataset_BUSI_with_GT/val_dataset')\n",
    "    ## upload test_dataset to minio\n",
    "    print('Uploading test_dataset to minio...')\n",
    "    upload_local_directory_to_minio('/tmp/test_dataset', minio_bucket, 'datasets/breast_cancer_segmentation/Dataset_BUSI_with_GT/test_dataset')\n",
    "\n",
    "\n",
    "\n",
    "def model_building(\n",
    "    epoch: int,\n",
    ") -> NamedTuple('Output', [('mlpipeline_ui_metadata', 'UI_metadata'),('mlpipeline_metrics', 'Metrics')]):\n",
    "    \"\"\"\n",
    "    Build the model with Keras API\n",
    "    Export model parameters\n",
    "    \"\"\"\n",
    "\n",
    "    import os\n",
    "    import json\n",
    "    import keras\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from glob import glob\n",
    "    import tensorflow as tf\n",
    "\n",
    "    # Model \n",
    "    from keras.models import Model\n",
    "    from keras.layers import Layer\n",
    "    from keras.layers import Conv2D\n",
    "    from keras.layers import Dropout\n",
    "    from keras.layers import UpSampling2D\n",
    "    from keras.layers import concatenate\n",
    "    from keras.layers import Add\n",
    "    from keras.layers import Multiply\n",
    "    from keras.layers import Input\n",
    "    from keras.layers import MaxPool2D\n",
    "    from keras.layers import BatchNormalization\n",
    "    \n",
    "    # Callbacks \n",
    "    from keras.callbacks import Callback\n",
    "    from keras.callbacks import EarlyStopping\n",
    "    from keras.callbacks import ModelCheckpoint\n",
    "    # Metrics\n",
    "    from keras.metrics import MeanIoU\n",
    "    from minio import Minio\n",
    "    \n",
    "    minio_client = Minio(\n",
    "        \"<minio_ep>\",\n",
    "        access_key=\"<minio_accK>\",\n",
    "        secret_key=\"<minio_secK>\",\n",
    "        secure=False\n",
    "    )\n",
    "    minio_bucket = \"kubeflow\"\n",
    "    \n",
    "    ## load image and mask\n",
    "    def load_image_and_mask(folder_path):\n",
    "    \n",
    "        image_file = tf.strings.join([folder_path, \"image.png\"], separator=\"/\")\n",
    "        mask_file = tf.strings.join([folder_path, \"mask.png\"], separator=\"/\")\n",
    "        \n",
    "        # load image\n",
    "        image = tf.io.read_file(image_file)\n",
    "        image = tf.image.decode_jpeg(image, channels=3)\n",
    "        image = tf.image.resize(image, [256, 256])\n",
    "        image = tf.cast(image, tf.float32) / 255.0\n",
    "        \n",
    "        # load mask\n",
    "        mask = tf.io.read_file(mask_file)\n",
    "        mask = tf.image.decode_png(mask, channels=1)\n",
    "        mask = tf.image.resize(mask, [256, 256], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "        mask = tf.cast(mask, tf.float32) / 255.0\n",
    "\n",
    "        return image, mask\n",
    "    def configure_for_performance(ds):\n",
    "        ds = ds.cache()\n",
    "        ds = ds.shuffle(buffer_size=1000)\n",
    "        ds = ds.batch(8)\n",
    "        ds = ds.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "        \n",
    "        return ds\n",
    "    ## get train_dataset\n",
    "    ### minio get\n",
    "    print('Downloading training data from minio...')\n",
    "    for f in minio_client.list_objects(minio_bucket, prefix=\"datasets/breast_cancer_segmentation/Dataset_BUSI_with_GT/train_dataset/\",recursive=True):\n",
    "        save_path = f.object_name.replace('datasets/breast_cancer_segmentation/Dataset_BUSI_with_GT', '/tmp')\n",
    "        minio_client.fget_object(minio_bucket, f.object_name, save_path)\n",
    "    ### train dataset /tmp/train_dataset/*\n",
    "    train_ds = tf.data.Dataset.list_files('/tmp/train_dataset/*')\n",
    "    train_ds = train_ds.map(load_image_and_mask)\n",
    "    train_ds = configure_for_performance(train_ds)\n",
    "    ## get val_dataset\n",
    "    print('Downloading validation data from minio...')\n",
    "    for f in minio_client.list_objects(minio_bucket, prefix=\"datasets/breast_cancer_segmentation/Dataset_BUSI_with_GT/val_dataset/\",recursive=True):\n",
    "        save_path = f.object_name.replace('datasets/breast_cancer_segmentation/Dataset_BUSI_with_GT', '/tmp')\n",
    "        minio_client.fget_object(minio_bucket, f.object_name, save_path)\n",
    "    ### minio get\n",
    "    val_ds = tf.data.Dataset.list_files('/tmp/val_dataset/*')\n",
    "    val_ds = val_ds.map(load_image_and_mask)\n",
    "    val_ds = configure_for_performance(val_ds)\n",
    "    ## get test_dataset\n",
    "    ### minio get\n",
    "    print('Downloading test data from minio...')\n",
    "    for f in minio_client.list_objects(minio_bucket, prefix=\"datasets/breast_cancer_segmentation/Dataset_BUSI_with_GT/test_dataset/\",recursive=True):\n",
    "        save_path = f.object_name.replace('datasets/breast_cancer_segmentation/Dataset_BUSI_with_GT', '/tmp')\n",
    "        minio_client.fget_object(minio_bucket, f.object_name, save_path)\n",
    "\n",
    "    ## model layers\n",
    "    ### Encoder\n",
    "    class EncoderBlock(Layer):\n",
    "\n",
    "        def __init__(self, filters, rate, pooling=True, **kwargs):\n",
    "            super(EncoderBlock, self).__init__(**kwargs)\n",
    "\n",
    "            self.filters = filters\n",
    "            self.rate = rate\n",
    "            self.pooling = pooling\n",
    "\n",
    "            self.c1 = Conv2D(filters, kernel_size=3, strides=1, padding='same', activation='relu', kernel_initializer='he_normal')\n",
    "            self.drop = Dropout(rate)\n",
    "            self.c2 = Conv2D(filters, kernel_size=3, strides=1, padding='same', activation='relu', kernel_initializer='he_normal')\n",
    "            self.pool = MaxPool2D()\n",
    "\n",
    "        def call(self, X):\n",
    "            x = self.c1(X)\n",
    "            x = self.drop(x)\n",
    "            x = self.c2(x)\n",
    "            if self.pooling:\n",
    "                y = self.pool(x)\n",
    "                return y, x\n",
    "            else:\n",
    "                return x\n",
    "\n",
    "        def get_config(self):\n",
    "            base_config = super().get_config()\n",
    "            return {\n",
    "                **base_config,\n",
    "                \"filters\":self.filters,\n",
    "                'rate':self.rate,\n",
    "                'pooling':self.pooling\n",
    "            }\n",
    "    \n",
    "    ### decoder\n",
    "    class DecoderBlock(Layer):\n",
    "\n",
    "        def __init__(self, filters, rate, **kwargs):\n",
    "            super(DecoderBlock, self).__init__(**kwargs)\n",
    "\n",
    "            self.filters = filters\n",
    "            self.rate = rate\n",
    "\n",
    "            self.up = UpSampling2D()\n",
    "            self.net = EncoderBlock(filters, rate, pooling=False)\n",
    "\n",
    "        def call(self, X):\n",
    "            X, skip_X = X\n",
    "            x = self.up(X)\n",
    "            c_ = concatenate([x, skip_X])\n",
    "            x = self.net(c_)\n",
    "            return x\n",
    "\n",
    "        def get_config(self):\n",
    "            base_config = super().get_config()\n",
    "            return {\n",
    "                **base_config,\n",
    "                \"filters\":self.filters,\n",
    "                'rate':self.rate,\n",
    "            }      \n",
    "    \n",
    "    ### Attention Gate\n",
    "    class AttentionGate(Layer):\n",
    "\n",
    "        def __init__(self, filters, bn, **kwargs):\n",
    "            super(AttentionGate, self).__init__(**kwargs)\n",
    "\n",
    "            self.filters = filters\n",
    "            self.bn = bn\n",
    "\n",
    "            self.normal = Conv2D(filters, kernel_size=3, padding='same', activation='relu', kernel_initializer='he_normal')\n",
    "            self.down = Conv2D(filters, kernel_size=3, strides=2, padding='same', activation='relu', kernel_initializer='he_normal')\n",
    "            self.learn = Conv2D(1, kernel_size=1, padding='same', activation='sigmoid')\n",
    "            self.resample = UpSampling2D()\n",
    "            self.BN = BatchNormalization()\n",
    "\n",
    "        def call(self, X):\n",
    "            X, skip_X = X\n",
    "\n",
    "            x = self.normal(X)\n",
    "            skip = self.down(skip_X)\n",
    "            x = Add()([x, skip])\n",
    "            x = self.learn(x)\n",
    "            x = self.resample(x)\n",
    "            f = Multiply()([x, skip_X])\n",
    "            if self.bn:\n",
    "                return self.BN(f)\n",
    "            else:\n",
    "                return f\n",
    "\n",
    "\n",
    "        def get_config(self):\n",
    "            base_config = super().get_config()\n",
    "            return {\n",
    "                **base_config,\n",
    "                \"filters\":self.filters,\n",
    "                \"bn\":self.bn\n",
    "            }    \n",
    "\n",
    "    ### Callback (option)\n",
    "    # pass\n",
    "\n",
    "    ### Attention U-Net\n",
    "    # Inputs\n",
    "    input_layer = Input(shape=(256,256,3)) ## `images` is input data\n",
    "\n",
    "    # Encoder\n",
    "    p1, c1 = EncoderBlock(32,0.1, name=\"Encoder1\")(input_layer)\n",
    "    p2, c2 = EncoderBlock(64,0.1, name=\"Encoder2\")(p1)\n",
    "    p3, c3 = EncoderBlock(128,0.2, name=\"Encoder3\")(p2)\n",
    "    p4, c4 = EncoderBlock(256,0.2, name=\"Encoder4\")(p3)\n",
    "\n",
    "    # Encoding\n",
    "    encoding = EncoderBlock(512,0.3, pooling=False, name=\"Encoding\")(p4)\n",
    "\n",
    "    # Attention + Decoder\n",
    "\n",
    "    a1 = AttentionGate(256, bn=True, name=\"Attention1\")([encoding, c4])\n",
    "    d1 = DecoderBlock(256,0.2, name=\"Decoder1\")([encoding, a1])\n",
    "\n",
    "    a2 = AttentionGate(128, bn=True, name=\"Attention2\")([d1, c3])\n",
    "    d2 = DecoderBlock(128,0.2, name=\"Decoder2\")([d1, a2])\n",
    "\n",
    "    a3 = AttentionGate(64, bn=True, name=\"Attention3\")([d2, c2])\n",
    "    d3 = DecoderBlock(64,0.1, name=\"Decoder3\")([d2, a3])\n",
    "\n",
    "\n",
    "    a4 = AttentionGate(32, bn=True, name=\"Attention4\")([d3, c1])\n",
    "    d4 = DecoderBlock(32,0.1, name=\"Decoder4\")([d3, a4])\n",
    "\n",
    "    # Output \n",
    "    output_layer = Conv2D(1, kernel_size=1, activation='sigmoid', padding='same')(d4)\n",
    "\n",
    "    # Model\n",
    "    model = Model(\n",
    "        inputs=[input_layer],\n",
    "        outputs=[output_layer]\n",
    "    )\n",
    "\n",
    "    # Compile\n",
    "    model.compile(\n",
    "        loss='binary_crossentropy',\n",
    "        optimizer='adam',\n",
    "        metrics=['accuracy', MeanIoU(num_classes=2, name='IoU')]\n",
    "    )\n",
    "\n",
    "    # Callbacks optional\n",
    "    # cb = [\n",
    "    #     EarlyStopping(patience=3, restore_best_weight=True), \n",
    "    #     ModelCheckpoint(\"AttentionCustomUNet.h5\", save_best_only=True),\n",
    "    #     # ShowProgress()\n",
    "    # ]\n",
    "    ## model summaryu\n",
    "    stringlist = []\n",
    "    model.summary(print_fn=lambda x: stringlist.append(x))\n",
    "    metric_model_summary = \"\\n\".join(stringlist)\n",
    "\n",
    "    # Config Training\n",
    "\n",
    "    # Training\n",
    "    history = model.fit(\n",
    "        train_ds,\n",
    "        validation_data=val_ds,\n",
    "        epochs=epoch, # 15 will be enough for a good Model for better model go with 20+\n",
    "        # callbacks=cb # optional\n",
    "    )\n",
    "    \n",
    "    ## TEST Stage\n",
    "    ### test dataset /tmp/test_dataset/*\n",
    "    test_ds = tf.data.Dataset.list_files('/tmp/test_dataset/*')\n",
    "    test_ds = test_ds.map(load_image_and_mask)\n",
    "    test_ds = configure_for_performance(test_ds)\n",
    "    loss, acc, iou = model.evaluate(test_ds)\n",
    "    \n",
    "    \n",
    "    ## metadata\n",
    "    metadata = {\n",
    "        \"outputs\": [\n",
    "            {\n",
    "                'storage': 'inline',\n",
    "                'source': '''# Model Overview\n",
    "## Model Summary\n",
    "\n",
    "```\n",
    "{}\n",
    "```\n",
    "\n",
    "## Model Performance\n",
    "\n",
    "**Accuracy**: {}\n",
    "**Loss**: {}\n",
    "**IoU**: {}\n",
    "'''.format(metric_model_summary,acc,loss,iou),\n",
    "                'type': 'markdown',\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    metrics = {\n",
    "      'metrics': [{\n",
    "          'name': 'Accuracy',\n",
    "          'numberValue':  float(acc),\n",
    "          'format' : \"PERCENTAGE\"\n",
    "        },{\n",
    "          'name': 'Loss',\n",
    "          'numberValue':  float(loss),\n",
    "          'format' : \"RAW\"\n",
    "        }\n",
    "        ]}\n",
    "    \n",
    "    ### Save model to minIO\n",
    "    print('start saving model')\n",
    "    keras.models.save_model(model, \"/tmp/attention_unet\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def upload_local_directory_to_minio(local_path, bucket_name, minio_path):\n",
    "        assert os.path.isdir(local_path)\n",
    "\n",
    "        for local_file in glob(local_path + '/**'):\n",
    "            local_file = local_file.replace(os.sep, \"/\") # Replace \\ with / on Windows\n",
    "            if not os.path.isfile(local_file):\n",
    "                upload_local_directory_to_minio(\n",
    "                    local_file, bucket_name, minio_path + \"/\" + os.path.basename(local_file))\n",
    "            else:\n",
    "                remote_path = os.path.join(\n",
    "                    minio_path, local_file[1 + len(local_path):])\n",
    "                remote_path = remote_path.replace(\n",
    "                    os.sep, \"/\")  # Replace \\ with / on Windows\n",
    "                minio_client.fput_object(bucket_name, remote_path, local_file)\n",
    "\n",
    "    upload_local_directory_to_minio(\"/tmp/attention_unet\",minio_bucket,\"models/attention_unet/1\") # 1 for version 1\n",
    "    \n",
    "    print(\"Saved model to minIO\")\n",
    "    \n",
    "    from collections import namedtuple\n",
    "    output = namedtuple('output', ['mlpipeline_ui_metadata', 'mlpipeline_metrics'])\n",
    "    return output(json.dumps(metadata),json.dumps(metrics))\n",
    "\n",
    "component_split_data = components.create_component_from_func(split_data,base_image=\"python:3.10.0\",\n",
    "                                                            packages_to_install=['scikit-learn','minio','numpy','pillow'])\n",
    "component_model_building = components.create_component_from_func(model_building,base_image=\"tensorflow/tensorflow:2.9.3\",\n",
    "                                                            packages_to_install=['scikit-learn','minio','pandas','numpy','tf_explain'])\n",
    "\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name='breast-cancer-segmentation',\n",
    "    description='example pipeline for breast cancer segmentation'\n",
    ")\n",
    "def output_test( epochs):\n",
    "    \n",
    "    now = datetime.now()\n",
    "    v = now.strftime(\"%Y%m%d%H%M%S\")\n",
    "    minio_bucket = \"kubeflow\"\n",
    "    \n",
    "    step1 = component_split_data()\n",
    "    step2 = component_model_building(epochs)\n",
    "    step2.after(step1)\n",
    "    \n",
    "    seldon_deployment = {\n",
    "        \"apiVersion\": \"machinelearning.seldon.io/v1\",\n",
    "        \"kind\": \"SeldonDeployment\",\n",
    "        \"metadata\": {\n",
    "            \"name\": f\"breast-cancer-segmentation-{v}\",\n",
    "            \"namespace\": \"kubeflow-user-example-com\"\n",
    "        },\n",
    "        \"spec\": {\n",
    "            \"protocol\": \"seldon\",\n",
    "            \"predictors\": [\n",
    "                {\n",
    "                    \"name\": \"predictor\",\n",
    "                    \"replicas\": 1,\n",
    "                    \"graph\": {\n",
    "                        \"name\": \"classifier\",\n",
    "                        \"implementation\": \"TENSORFLOW_SERVER\",\n",
    "                        \"modelUri\": f\"s3://{minio_bucket}/models/attention_unet\",\n",
    "                        \"envSecretRefName\": \"seldon-init-container-secret\"\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    step3 = dsl.ResourceOp(\n",
    "        name=f'seldon-deployment-{v}',\n",
    "        k8s_resource=seldon_deployment,\n",
    "        action=\"create\",\n",
    "        attribute_outputs={\"name\": \"{.metadata.name}\"}\n",
    "    )\n",
    "    step3.after(step2)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    kubeflow_gateway_endpoint = \"<kubeflow-gateway-endpoint>\" # e.g. 172.0.0.1\n",
    "    authservice_session_cookie = \"<authservice_session_cookie>\"\n",
    "    \n",
    "    client = kfp.Client(host=f\"https://{kubeflow_gateway_endpoint}/pipeline\",\n",
    "                        cookies=f\"authservice_session={authservice_session_cookie}\",\n",
    "                        ssl_ca_cert=\"cert/tls.crt\") # need to store tls.crt before running the pipeline\n",
    "\n",
    "    arguments = {\n",
    "        \"epochs\": 4\n",
    "    }\n",
    "\n",
    "    client.create_run_from_pipeline_func(output_test,arguments=arguments,experiment_name=\"breast-cancer-segmentation\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
